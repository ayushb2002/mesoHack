{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "09562824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from statistics import mean\n",
    "from random import shuffle\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9c17b",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Analysis - Data being fetched from NLTK library\n",
    "\n",
    "* names: A list of common English names compiled by Mark Kantrowitz\n",
    "* stopwords: A list of really common words, like articles, pronouns, prepositions, and conjunctions\n",
    "* state_union: A sample of transcribed State of the Union addresses by different US presidents, compiled by Kathleen Ahrens\n",
    "* twitter_samples: A list of social media phrases posted to Twitter\n",
    "* movie_reviews: Two thousand movie reviews categorized by Bo Pang and Lillian Lee\n",
    "* averaged_perceptron_tagger: A data model that NLTK uses to categorize words into their part of speech\n",
    "* vader_lexicon: A scored list of words and jargon that NLTK references when performing sentiment analysis, created by C.J. Hutto and Eric Gilbert\n",
    "* punkt: A data model created by Jan Strunk that NLTK uses to split full texts into word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b53460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download([\n",
    "#      \"names\",\n",
    "#      \"stopwords\",\n",
    "#      \"state_union\",\n",
    "#      \"twitter_samples\",\n",
    "#      \"movie_reviews\",\n",
    "#      \"averaged_perceptron_tagger\",\n",
    "#      \"vader_lexicon\",\n",
    "#      \"punkt\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74288cc",
   "metadata": {},
   "source": [
    "### Fetching script from state_union collection from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad60923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197a0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c6e90",
   "metadata": {},
   "source": [
    "### Stopwords occur too often in the sentences, thus leaving a negative impact on the sentiment analysis, thus we remove them from our scipts. Also converting all the script to lowecase to avoid mixed cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f383b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba0332",
   "metadata": {},
   "source": [
    "### Now we determine the frequency of words and create a dictionary of key->words and value->frequency and run analysis on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59e379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25d18f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('must', 1568), ('people', 1291), ('world', 1128)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bed30b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   must  people   world    year America      us \n",
      "   1568    1291    1128    1097    1076    1049 \n"
     ]
    }
   ],
   "source": [
    "fd.tabulate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2cc68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_fd = nltk.FreqDist([w.lower() for w in fd]) # Converts all types of cases to lower case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5a75d",
   "metadata": {},
   "source": [
    "## Extracting Concordance and Collocations\n",
    "In the context of NLP, a `concordance` is a collection of word locations along with their context. You can use concordances to find:\n",
    "\n",
    "* How many times a word appears\n",
    "* Where each occurrence appears\n",
    "* What words surround each occurrence\n",
    "\n",
    "In NLTK, you can do this by calling `concordance()` To use it, you need an instance of the `nltk.Text` class, which can also be constructed with a word list.\n",
    "\n",
    "Another powerful feature of NLTK is its ability to quickly find `collocations` with simple function calls. Collocations are series of words that frequently appear together in a given text. In the State of the Union corpus, for example, youâ€™d expect to find the words United and States appearing next to each other very often. Those two words appearing together is a collocation.\n",
    "\n",
    "Collocations can be made up of two or more words. NLTK provides classes to handle several types of collocations:\n",
    "\n",
    "* Bigrams: Frequent two-word combinations\n",
    "* Trigrams: Frequent three-word combinations\n",
    "* Quadgrams: Frequent four-word combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dac099f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Luck is not a matter of chance. Lucky You!\n",
    "Thomas Jefferson once said, \"I'm a great believer in luck, and I find the harder I work, the more I have of it.\" What, though, is luck? Webster's dictionary suggests that luck is the \"events or circumstances that operate for or against an individual.\"\n",
    "In truth, luck has nothing to do with something operating for or against you. Luck is not a matter of chance. It is a matter of being open to new experiences, perseverance, hard work, and positive thinking.\n",
    "When seventeen-year-old Steven Spielberg spent some time with his cousin in the summer of 1965, they toured Universal pictures. The tram stopped at none of the sound stages. Spielberg snuck off on a bathroom break to watch a bit of the real action. When he encountered an unfamiliar face who demanded to know what he was doing, he told him his story. The man turned out to be the head of the editorial department. Spielberg got a pass to the lot for the very next day and showed a very impressed Chuck Silvers four of his eight-millimeter films. This event was the foot in the door Spielberg needed to start squatting on the lot, a decision that led to his first contract with Universal Studios.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad74aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "words: list[str] = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13ed8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac00a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "144f3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94527f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = text.vocab() # same as FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11524a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     luck Spielberg    matter \n",
      "        4         4         3 \n"
     ]
    }
   ],
   "source": [
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "98013e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_list = text.concordance_list(\"luck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1727e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Luck matter chance Lucky Thomas Jefferson\n",
      "Thomas Jefferson said great believer luck find harder work though luck Webster\n",
      "eliever luck find harder work though luck Webster dictionary suggests luck eve\n",
      "ugh luck Webster dictionary suggests luck events circumstances operate individ\n",
      "rcumstances operate individual truth luck nothing something operating Luck mat\n",
      "uth luck nothing something operating Luck matter chance matter open new experi\n"
     ]
    }
   ],
   "source": [
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a05bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "80b56c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Luck', 'matter', 'chance'), 2), (('matter', 'chance', 'Lucky'), 1)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.ngram_fd.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04f4f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ('Luck', 'matter', 'chance') ('matter', 'chance', 'Lucky') \n",
      "                            2                             1 \n"
     ]
    }
   ],
   "source": [
    "finder.ngram_fd.tabulate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c685b2",
   "metadata": {},
   "source": [
    "## Using NLTKâ€™s Pre-Trained Sentiment Analyzer\n",
    "NLTK already has a built-in, pretrained sentiment analyzer called VADER (Valence Aware Dictionary and sEntiment Reasoner).\n",
    "\n",
    "Since VADER is pretrained, you can get results more quickly than with many other analyzers. However, VADER is best suited for language used in social media, like short sentences with some slang and abbreviations. Itâ€™s less accurate when rating longer, structured sentences, but itâ€™s often a good launching point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd7d2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f9c7a",
   "metadata": {},
   "source": [
    "The negative, neutral, and positive scores are related: They all add up to 1 and canâ€™t be negative. The compound score is calculated differently. Itâ€™s not just an average, and it can range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4bb3ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive(string: str):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(string)[\"compound\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c9fce1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_positive(\"The lion ate the cat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3eeba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_positive(\"She escaped successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0840f",
   "metadata": {},
   "source": [
    "## Customizing NLTKâ€™s Sentiment Analysis - Selecting useful features!\n",
    "NLTK offers a few built-in classifiers that are suitable for various types of analyses, including sentiment analysis. The trick is to figure out which properties of your dataset are useful in classifying each piece of data into your desired categories.\n",
    "\n",
    "In the world of machine learning, these data properties are known as features, which you must reveal and select as you work with your data. While this tutorial wonâ€™t dive too deeply into feature selection and feature engineering, youâ€™ll be able to see their effects on the accuracy of classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3634ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1736a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.isalpha():\n",
    "                if word.lower() in positive_words:\n",
    "                    wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "40ea61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4127fc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_compound': 1.26714,\n",
       " 'mean_positive': 0.20626666666666668,\n",
       " 'wordcount': 62}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c4bead4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(text), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(text), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "33a1bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           mean_compound = 1.26714           neg : pos    =      1.0 : 1.0\n",
      "           mean_positive = 0.20626666666666668    neg : pos    =      1.0 : 1.0\n",
      "               wordcount = 62                neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4639f42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49266666666666664"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "937bce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a4b1d",
   "metadata": {},
   "source": [
    "#### For each scikit-learn classifier, call nltk.classify.SklearnClassifier to create a usable NLTK classifier that can be trained and evaluated exactly like youâ€™ve seen before with nltk.NaiveBayesClassifier and its other built-in classifiers. The .train() and .accuracy() methods should receive different portions of the same list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b883aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.60% - BernoulliNB\n",
      "49.60% - ComplementNB\n",
      "49.60% - MultinomialNB\n",
      "50.40% - KNeighborsClassifier\n",
      "49.60% - DecisionTreeClassifier\n",
      "49.60% - RandomForestClassifier\n",
      "49.60% - LogisticRegression\n",
      "49.60% - MLPClassifier\n",
      "49.60% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    classifier.train(features[:train_count])\n",
    "    accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f953fc2",
   "metadata": {},
   "source": [
    "#### Highest score is with KNeighbours Classifier, thus trying with full feature set on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2de8e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.79% - KNN\n"
     ]
    }
   ],
   "source": [
    "train_count = int(len(features) *0.95) \n",
    "shuffle(features)\n",
    "\n",
    "classifier = nltk.classify.SklearnClassifier(KNeighborsClassifier())\n",
    "classifier.train(features[:train_count])\n",
    "accuracy = nltk.classify.accuracy(classifier, features[len(features)-train_count:])\n",
    "print(F\"{accuracy:.2%} - KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51afdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
